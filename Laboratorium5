

from sklearn.decomposition import PCA
from sklearn import datasets
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns

# Wczytaj przykładowy zbiór danych - dane dotyczące trzech gatunków Irysów
iris = datasets.load_iris()

# Podzielmy zbiór na cechy oraz etykiety
# Zostawiamy tym razem wszystkie cechy - będziemy próbować odgadnąć które cechy są najważniejsze
X = iris.data
y = iris.target

# Inicjalizacja. Można od razu wypełnić n_components, na razie wykorzystujemy wszystkie cechy 
#pca = PCA(n_components=2)

pca = PCA()
pca.fit(X)

print(pca.components_)
print(pca.n_components_)
print(pca.explained_variance_ratio_)

# konwersja na obiekt pandas.DataFrame
iris_df = pd.DataFrame(iris['data'], columns=iris['feature_names'])

# funkcja która nam zamieni wartości 0, 1, 2 na pełny opis tekstowy dla gatunku
targets = map(lambda x: iris['target_names'][x], iris['target'] )

# doklejenie informacji o gatunku do reszty dataframe
iris_df['species'] = np.array(list(targets))

# wykres
sns.pairplot(iris_df, hue='species')
plt.show()

pca_limit = PCA(n_components = 1)

X_new = pca_limit.fit_transform(X)

print(pca_limit.n_components_)
print(pca_limit.explained_variance_ratio_)

# Po użyciu funkcji transform (lub fit_transform) dekompozycja pozostawiła nam tylko liczbę cech, którą skonfigurowaliśmy
# Dodatkowo została od nich odjęta średnia, więc dane zawierają tylko wariancję

X_new[:5]

plt.scatter(X_new, y)
plt.show()


#ZADANIE1

#Im większa rozpiętość (czyli wariancja) tym lepsza zmienna (cecha) 
#bo niesie w sobie więcej informacji. I odwrotnie, jeśli wartość własna 
#jest bardzo mała, wariancja jest bardzo mała to cecha wnosi niewiele informacji.
#Według mnie najlepszą cechą byłaby nowa cecha "Powierzchnia płatka" czyli 
#petal width razy petal length ponieważ posiada najbardziej
#rozpięte wartosci  a co za tym idzie największą wariancje.
